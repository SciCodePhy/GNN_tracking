{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabf0241",
   "metadata": {},
   "source": [
    "# Task: Edge classification with pytorch geometric\n",
    "### Chenguang Guan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a9a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Sigmoid\n",
    "\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650f8f0",
   "metadata": {},
   "source": [
    "## A. Dataset Preparation\n",
    "We can either directly load the dataset and pass it to a list, or define a custom Dataset class as the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac4909",
   "metadata": {},
   "source": [
    "### I. The Naive Way\n",
    "The advantage point is that we can easily seperate the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7bf94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_dataset(path: Path):\n",
    "    total_graphs = list()\n",
    "    for i in range(10):\n",
    "        subdir = \"batch_1_\" + str(i) + \"/\"\n",
    "        total_graphs += list(path.glob(subdir + \"*.pt\"))\n",
    "    dataset = list()\n",
    "    for j in range(len(total_graphs)):\n",
    "        dataset.append(torch.load(total_graphs[j]))\n",
    "    return dataset\n",
    "dataset = naive_dataset(Path(\"./data_gsoc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25cc672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "train_loader = DataLoader(dataset[:int(9/10*len(dataset))], batch_size=32)\n",
    "test_loader = DataLoader(dataset[int(9/10*len(dataset)):], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d76c67",
   "metadata": {},
   "source": [
    "### II. Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a60eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path: Path):\n",
    "        super().__init__()\n",
    "        total_graphs = list()\n",
    "        for i in range(10):\n",
    "            subdir = \"batch_1_\" + str(i) + \"/\"\n",
    "            total_graphs += list(path.glob(subdir + \"*.pt\"))\n",
    "        \n",
    "        #random.shuffle(total_graphs)\n",
    "        self.graphs = total_graphs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.load(self.graphs[idx])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.graphs)\n",
    "        \n",
    "#dataset = MyDataset(Path(\"./data_gsoc\"))\n",
    "#dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659f4bc",
   "metadata": {},
   "source": [
    "### III. Attributes of The Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f1482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[168, 6], edge_index=[2, 1256], edge_attr=[1256, 4], y=[1256])\n",
      "num of nodes: 168\n",
      "num of edges: 1256\n",
      "num of node features: 6\n",
      "num of edge features: 4\n",
      "Whether it's a directed graph? True\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(\"num of nodes:\",dataset[0].num_nodes)\n",
    "print(\"num of edges:\",dataset[0].num_edges)\n",
    "print(\"num of node features:\",dataset[0].num_node_features)\n",
    "print(\"num of edge features:\",dataset[0].num_edge_features)\n",
    "print(\"Whether it's a directed graph?\",dataset[0].is_directed())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee538863",
   "metadata": {},
   "source": [
    "This is a directed graph with 6-d node features and 4-d edge features, and edge index is written in the Coordinate format (COO):\n",
    "$$ G = (X, R, I) $$\n",
    "Where,\n",
    "$$ X = R^{n_{nodes}\\times 6} $$\n",
    "$$ R = R^{n_{edges}\\times 4} $$\n",
    "$$ I = N^{2\\times n_{edges}} $$\n",
    "$I[0,i]$ is the source node of the i-th edge, and $I[1,i]$ is the target node of the i-th edge.\n",
    "While we also have traning target $Y=\\{0,1\\}^{n_{edges}}$, which is tha ground truth of the edge classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdae2a",
   "metadata": {},
   "source": [
    "## B. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa57d1a",
   "metadata": {},
   "source": [
    "The main idea of graph neural network is to update the node embedding vectors and edge embedding vectors based on locality and message passing.\n",
    "The general form of the updating is:\n",
    "$$x_i^{(t)}=\\phi_{\\text {node }}^{(t)}\\left(x_i^{(t-1)}, \\underset{j \\in N(i)}{\\square} \\phi_{\\text {message }}^{(t)}\\left(x_i^{(t-1)}, x_j^{(t-1)}, a_{i j}^{(t-1)}\\right)\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d043b",
   "metadata": {},
   "source": [
    "### Model-I: Interaction Networks\n",
    "This model comes from \"DeZoort, G., Thais, S., Duarte, J. et al. Charged Particle Tracking via Edge-Classifying Interaction Networks. Comput Softw Big Sci 5, 26 (2021). https://doi.org/10.1007/s41781-021-00073-z\"\n",
    "\n",
    "In this model:\n",
    "1. We take the \"aggregation\" as \"add\": $\\underset{j \\in N(i)}{\\square} = \\sum_{j \\in N(i)} $.\n",
    "2. We assign $\\phi_{\\text {message }}^{(t)}\\left(x_i^{(t-1)}, x_j^{(t-1)}, e_{i j}^{(t-1)}\\right) $ to $a_{ij}^{(t)}$, which means that $a_{ij}^{(t)}= \\phi_{\\text {message }}^{(t)}\\left(x_i^{(t-1)}, x_j^{(t-1)}, a_{i j}^{(t-1)}\\right)$ and $x_i^{(t)}=\\phi_{\\text {node }}^{(t)}\\left(x_i^{(t-1)},\\underset{j \\in N(i)}{\\square}a_{i j}^{(t-1)}\\right) $.\n",
    "\n",
    "In the literature:\n",
    "1. The authors take $\\phi_{\\text {message }} \\rightarrow \\phi_{R, 1}$ and $\\phi_{\\text {node }} \\rightarrow \\phi_O$ as MLP.\n",
    "2. The authors take only one time step (only update the embedding vectors one time)\n",
    "$$a_{i j}^{(1)}=\\phi_{R, 1}\\left(x_i^{(0)}, x_j^{(0)}, a_{i j}^{(0)}\\right)$$\n",
    "$$x_i^{(1)}=\\phi_O\\left(x_i^{(0)}, \\sum_{j \\in N(i)} a_{i j}^{(1)}\\right)$$\n",
    "\n",
    "3. We also need an extra layer to transform the embedding vectors to classification results (also called weights):\n",
    "$$w_{i j}^{(1)}:=\\phi_{R, 2}\\left(x_i^{(1)}, x_j^{(1)}, a_{i j}^{(1)}\\right)$$\n",
    "\n",
    "4. $\\phi_{R, 1}$ and $\\phi_{R, 2}$ are called Relational Models, and $\\phi_{O}$ is called Object Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7384ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Layer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(MLP_Layer, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, m):\n",
    "        return self.layers(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2281c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetwork(MessagePassing):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(InteractionNetwork, self).__init__(aggr='add', \n",
    "                                                 flow='source_to_target')\n",
    "        self.R1 = MLP_Layer(16, 4, hidden_size)\n",
    "        self.O = MLP_Layer(10, 6, hidden_size)\n",
    "        self.R2 = MLP_Layer(16, 1, hidden_size)\n",
    "        self.E: Tensor = Tensor()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_attr: Tensor)\n",
    "        x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "        m2 = torch.cat([x_tilde[edge_index[1]],\n",
    "                        x_tilde[edge_index[0]],\n",
    "                        self.E], dim=1)\n",
    "        return torch.sigmoid(self.R2(m2))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # x_i --> incoming\n",
    "        # x_j --> outgoing        \n",
    "        m1 = torch.cat([x_i, x_j, edge_attr], dim=1)\n",
    "        self.E = self.R1(m1)\n",
    "        return self.E\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        c = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.O(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b18c0",
   "metadata": {},
   "source": [
    "### Multi-Layer Interaction Network\n",
    "We can also stack more layers of InteractionNetwork before moving to the edge weighting stage ($w_{i j}^{(1)}=\\phi_{R, 2}\\left(x_i^{(1)}, x_j^{(1)}, a_{i j}^{(1)}\\right) $):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "195ca072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetwork_wo_weight(MessagePassing):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(InteractionNetwork_wo_weight, self).__init__(aggr='add', \n",
    "                                                 flow='source_to_target')\n",
    "        self.R1 = MLP_Layer(16, 4, hidden_size)\n",
    "        self.O = MLP_Layer(10, 6, hidden_size)\n",
    "        #self.R2 = MLP_Layer(16, 1, hidden_size)\n",
    "        self.E: Tensor = Tensor()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_attr: Tensor)\n",
    "        x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "        return x_tilde, self.E\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # x_i --> incoming\n",
    "        # x_j --> outgoing        \n",
    "        m1 = torch.cat([x_i, x_j, edge_attr], dim=1)\n",
    "        self.E = self.R1(m1)\n",
    "        return self.E\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        c = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.O(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a493fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_IN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Multi_IN, self).__init__()\n",
    "        \n",
    "        #self.IN_layer = nn.Sequential(InteractionNetwork_wo_weight(hidden_size), InteractionNetwork_wo_weight(hidden_size))\n",
    "        self.R2 = MLP_Layer(16, 1, hidden_size)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor, num_layers = 1) -> Tensor:\n",
    "        for i in range(num_layers):\n",
    "            x, edge_attr = InteractionNetwork_wo_weight(hidden_size)(x=x,edge_index=edge_index,edge_attr=edge_attr)\n",
    "        m2 = torch.cat([x[edge_index[1]], x[edge_index[0]], edge_attr], dim=1)\n",
    "        return torch.sigmoid(self.R2(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46cb6f",
   "metadata": {},
   "source": [
    "### DIY Models\n",
    "A lot of models do not consider edge features in their updating schemes, such as famous GCN (https://arxiv.org/abs/1609.02907) and Edge Convolution (https://arxiv.org/abs/1801.07829). Therefore, one naive idea is to directly concatenate the edge embedding vectors with node embedding vectors when doing message passing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae03b38",
   "metadata": {},
   "source": [
    "#### The Variant of Edge Convolution (DIY)\n",
    "The orginal Edge Convolution is defined as:\n",
    "$$x_i^{(k)}=\\max _{j \\in N(i)} h_{\\Theta}\\left(x_i^{(k-1)}, x_j^{(k-1)}-x_i^{(k-1)}\\right)$$\n",
    "\n",
    "Here, $h_{\\Theta}$ is also a MLP. The original Edge Convolution is suitable for the network without edge features.\n",
    "\n",
    "Therefore, we can modify the Edge Convolution and include node features into the message passing.\n",
    "$$a_{ij}^{(k)}= h_{\\Theta}\\left(x_i^{(k-1)}, x_j^{(k-1)}-x_i^{(k-1)},a_{ij}^{(k-1)}\\right)$$\n",
    "$$x_i^{(1)}=\\phi_O\\left(x_i^{(k)}, \\max _{j \\in N(i)} a_{i j}^{(k-1)}\\right)$$\n",
    "\n",
    "We also have two choices for the weights calculation:\n",
    "1. Same as Interacting Network:$$w_{i j}^{(k)}:=\\phi_{R, 2}\\left(x_i^{(k)}, x_j^{(k)}, a_{i j}^{(k)}\\right)$$\n",
    "2. Or Similar with Edge Convolution Layer: $$w_{i j}^{(k)}:=\\phi_{R, 2}\\left(x_i^{(k)}, x_j^{(k)} - x_i^{(k)}, a_{i j}^{(k)}\\right)$$\n",
    "\n",
    "Interacting network in the literature and our DIY Edge Convolution can be both included in the general framework of graph neural network. \n",
    "$$x_i^{(t)}=\\phi_{\\text {node }}^{(t)}\\left(x_i^{(t-1)}, \\underset{j \\in N(i)}{\\square} \\phi_{\\text {message }}^{(t)}\\left(x_i^{(t-1)}, x_j^{(t-1)}, a_{i j}^{(t-1)}\\right)\\right)$$\n",
    "\n",
    "The differences between them are:\n",
    "1. The form of $\\phi_{\\text {node }}$ is different.\n",
    "2. $\\underset{j \\in N(i)}{\\square}$ in Interacting Network is sum, while $\\underset{j \\in N(i)}{\\square}$ in DIY edge convolution is max."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a42b3",
   "metadata": {},
   "source": [
    "#### Type-I\n",
    "$$w_{i j}^{(k)}:=\\phi_{R, 2}\\left(x_i^{(k)}, x_j^{(k)}, a_{i j}^{(k)}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd67c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv_type1(MessagePassing):\n",
    "    def __init__(self,hidden_size):\n",
    "        super().__init__(aggr='max') #  \"Max\" aggregation.\n",
    "        self.mlp = Seq(Linear(16, 4),\n",
    "                       ReLU(),\n",
    "                       Linear(4, 4))\n",
    "        self.mlp_2 = Seq(Linear(10, 6),\n",
    "                       ReLU(),\n",
    "                       Linear(6, 6))\n",
    "        self.R2 = nn.Sequential(\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "        self.E: Tensor = Tensor()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "        m2 = torch.cat([x_tilde[edge_index[1]],\n",
    "                        x_tilde[edge_index[0]]-x_tilde[edge_index[1]],\n",
    "                        self.E], dim=1)\n",
    "        return torch.sigmoid(self.R2(m2))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # x_i has shape [E, in_channels]\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        tmp = torch.cat([x_i, x_j, edge_attr], dim=1)  # tmp has shape [E, in_channels]\n",
    "        self.E =self.mlp(tmp)\n",
    "        \n",
    "        return self.E\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        c = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.mlp_2(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b98e52",
   "metadata": {},
   "source": [
    "#### Type-II\n",
    "$$w_{i j}^{(k)}:=\\phi_{R, 2}\\left(x_i^{(k)}, x_j^{(k)} - x_i^{(k)}, a_{i j}^{(k)}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "786c5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv_type2(MessagePassing):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__(aggr='max') #  \"Max\" aggregation.\n",
    "        self.mlp = Seq(Linear(16, 4),\n",
    "                       ReLU(),\n",
    "                       Linear(4, 4))\n",
    "        self.mlp_2 = Seq(Linear(10, 6),\n",
    "                       ReLU(),\n",
    "                       Linear(6, 6))\n",
    "        self.R2 = nn.Sequential(\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "        self.E: Tensor = Tensor()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "        m2 = torch.cat([x_tilde[edge_index[1]],\n",
    "                        x_tilde[edge_index[0]]-x_tilde[edge_index[1]],\n",
    "                        self.E], dim=1)\n",
    "        return torch.sigmoid(self.R2(m2))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # x_i has shape [E, in_channels]\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        tmp = torch.cat([x_i, x_j - x_i, edge_attr], dim=1)  # tmp has shape [E, in_channels]\n",
    "        self.E =self.mlp(tmp)\n",
    "        \n",
    "        return self.E\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        c = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.mlp_2(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab7277",
   "metadata": {},
   "source": [
    "## C. Training and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4e719",
   "metadata": {},
   "source": [
    "Before moving to explicit models, we can define the general train loop function and test loop function. Here, we take the binary cross entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef931b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.binary_cross_entropy\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.x,X.edge_index,X.edge_attr)\n",
    "        #print(pred.shape)\n",
    "        #print(X.y.shape)\n",
    "        loss = loss_fn(pred.squeeze(), X.y, reduction='mean')\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X.x,X.edge_index,X.edge_attr)\n",
    "            test_loss += loss_fn(pred.squeeze(), X.y, reduction='mean').item()\n",
    "            correct += (pred.argmax(1) == X.y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2bf30",
   "metadata": {},
   "source": [
    "### 1. Training and Evaluation of single layer IN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215c1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "device = \"cpu\"\n",
    "model_1 = InteractionNetwork(hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9de5d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.351929  [   32/ 8096]\n",
      "loss: 0.375481  [ 3232/ 8096]\n",
      "loss: 0.388098  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.375170 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.349160  [   32/ 8096]\n",
      "loss: 0.373014  [ 3232/ 8096]\n",
      "loss: 0.385588  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.372890 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.346620  [   32/ 8096]\n",
      "loss: 0.370670  [ 3232/ 8096]\n",
      "loss: 0.383213  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.370744 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.344289  [   32/ 8096]\n",
      "loss: 0.368454  [ 3232/ 8096]\n",
      "loss: 0.380947  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.368706 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.342135  [   32/ 8096]\n",
      "loss: 0.366342  [ 3232/ 8096]\n",
      "loss: 0.378783  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.366780 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "optimizer_1 = torch.optim.SGD(model_1.parameters(), lr=learning_rate)\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model_1, loss_fn, optimizer_1)\n",
    "    test_loop(test_loader, model_1, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b2fa7",
   "metadata": {},
   "source": [
    "### 2. Training and Evaluation of multi layer IN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8feb1f",
   "metadata": {},
   "source": [
    "we need to modify our train loop function and test loop function (introducing num of layers) for multi layer IN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90b20c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_2(dataloader, model, loss_fn, optimizer, num_layers):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.x,X.edge_index,X.edge_attr,num_layers)\n",
    "        #print(pred.shape)\n",
    "        #print(X.y.shape)\n",
    "        loss = loss_fn(pred.squeeze(), X.y, reduction='mean')\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop_2(dataloader, model, loss_fn, num_layers):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X.x,X.edge_index,X.edge_attr, num_layers)\n",
    "            test_loss += loss_fn(pred.squeeze(), X.y, reduction='mean').item()\n",
    "            correct += (pred.argmax(1) == X.y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dc47fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "device = \"cpu\"\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ef1d214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.684882  [   32/ 8096]\n",
      "loss: 0.674749  [ 3232/ 8096]\n",
      "loss: 0.662488  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.655081 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.653246  [   32/ 8096]\n",
      "loss: 0.663603  [ 3232/ 8096]\n",
      "loss: 0.642421  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.628446 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.636243  [   32/ 8096]\n",
      "loss: 0.623932  [ 3232/ 8096]\n",
      "loss: 0.613959  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.605644 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.606629  [   32/ 8096]\n",
      "loss: 0.595179  [ 3232/ 8096]\n",
      "loss: 0.599072  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.586985 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.578913  [   32/ 8096]\n",
      "loss: 0.576923  [ 3232/ 8096]\n",
      "loss: 0.576522  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.567949 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model_2 = Multi_IN(hidden_size).to(device)\n",
    "optimizer_2 = torch.optim.SGD(model_2.parameters(), lr=learning_rate)\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop_2(train_loader, model_2, loss_fn, optimizer_2, num_layers)\n",
    "    test_loop_2(test_loader, model_2, loss_fn, num_layers)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753630ff",
   "metadata": {},
   "source": [
    "### 3. Training and Evaluation of DIY Edge Convolution (Type-I):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d339fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "device = \"cpu\"\n",
    "model_3 = EdgeConv_type1(hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b79ffbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.739243  [   32/ 8096]\n",
      "loss: 0.697415  [ 3232/ 8096]\n",
      "loss: 0.674982  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.665960 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.666597  [   32/ 8096]\n",
      "loss: 0.641850  [ 3232/ 8096]\n",
      "loss: 0.626733  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.618573 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.614422  [   32/ 8096]\n",
      "loss: 0.599718  [ 3232/ 8096]\n",
      "loss: 0.589220  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.581081 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.572656  [   32/ 8096]\n",
      "loss: 0.565215  [ 3232/ 8096]\n",
      "loss: 0.558832  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.550354 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.537920  [   32/ 8096]\n",
      "loss: 0.536471  [ 3232/ 8096]\n",
      "loss: 0.534633  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.525688 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "optimizer_3 = torch.optim.SGD(model_3.parameters(), lr=learning_rate)\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model_3, loss_fn, optimizer_3)\n",
    "    test_loop(test_loader, model_3, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c00ea",
   "metadata": {},
   "source": [
    "### 4. Training and Evaluation of DIY Edge Convolution (Type-II):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "963012dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "device = \"cpu\"\n",
    "model_4 = EdgeConv_type2(hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "267445fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.645674  [   32/ 8096]\n",
      "loss: 0.633662  [ 3232/ 8096]\n",
      "loss: 0.628654  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.621502 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.616622  [   32/ 8096]\n",
      "loss: 0.608760  [ 3232/ 8096]\n",
      "loss: 0.606546  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.599874 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.592948  [   32/ 8096]\n",
      "loss: 0.588337  [ 3232/ 8096]\n",
      "loss: 0.588244  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.581717 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.572993  [   32/ 8096]\n",
      "loss: 0.570952  [ 3232/ 8096]\n",
      "loss: 0.572633  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.566107 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.555742  [   32/ 8096]\n",
      "loss: 0.555816  [ 3232/ 8096]\n",
      "loss: 0.559052  [ 6432/ 8096]\n",
      "Test Error: \n",
      " Accuracy: 314565.8%, Avg loss: 0.552473 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "optimizer_4 = torch.optim.SGD(model_4.parameters(), lr=learning_rate)\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model_4, loss_fn, optimizer_4)\n",
    "    test_loop(test_loader, model_4, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77dedf",
   "metadata": {},
   "source": [
    "## D. Further Directions\n",
    "Due to the time limitation, I can not implement every ideas in the task. However, in the near future there are several directions I would like to contribute to:\n",
    "### Network Structure\n",
    "1. #### Next Nearest Neighbour:\n",
    "The first way to improve the network I would like to try is to include next nearest neighbor for the message passing. We can extend the $\\underset{j \\in N(i)}{\\square}$ to $\\underset{j \\in N(i)\\cup N.N.(i)}{\\square}$, where $N.N.(i)$ denotes next nearest neighbot of node-$i$.\n",
    "\n",
    "2. #### Graph Attention Network \n",
    "We can also try some framework beyond Graph Convolutional Network Scheme. For example, one promising direction is using Graph Attention Networkï¼ˆGAT) in the task.\n",
    "\n",
    "### Training Results (Bug!)\n",
    "The accuracy on test dataset of four models are all apprxomately equal to $31\\%$, which is very weird. There might be some bugs in the training or evaluation part of my code.\n",
    "\n",
    "### Evaluation\n",
    "I would like to utilize ROC curve (Receiver Operating Characteristic curve) and AUC score (Area Under the ROC Curve) in the further evaluation.\n",
    "\n",
    "### Class Imbalance Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fa9517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of class-0 882\n",
      "Num of class-1 374\n"
     ]
    }
   ],
   "source": [
    "num_0 = 0\n",
    "num_1 = 0\n",
    "for i in dataset[0].y:\n",
    "    if i == 0:\n",
    "        num_0 += 1\n",
    "    elif i == 1:\n",
    "        num_1 += 1\n",
    "    else:\n",
    "        print(\"error\")\n",
    "print(\"Num of class-0\",num_0)\n",
    "print(\"Num of class-1\",num_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e37da2",
   "metadata": {},
   "source": [
    "There is a significant class imbalance in our data. \n",
    "\n",
    "In the normal case, we can use under-sampling or over-sampling to fix the imbalanced class. However, in the graph problem this is tricky, because we can not arbitarily delete or add edges in a single graph.\n",
    "\n",
    "Therefore, we might need to find a way to enlarge or reduce each graph (introducing extra nodes and edges with $y=1$ or properly deleting existing nodes and edges with $y=0$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
